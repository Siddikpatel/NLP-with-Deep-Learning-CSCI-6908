{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkeRSdRkTXhw"
   },
   "source": [
    "# CSCI6908: Assignment 3 - Neural Machine Translation.\n",
    "\n",
    "**<span style=\"color:red\">Deadline: Monday, March 3rd, 2025</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J06PU1l6qISF"
   },
   "source": [
    "# Part 0: Introduction\n",
    "\n",
    "In this assignment, you will build, train, and evaluate a neural machine translation system. The system will be in the form of a seq2seq neural network, that takes as input a French sequence and translates it to English. Unlike the previous assignment where the whole system was built from scratch, you will now use `pytorch`, a Python framework for building deep learning models.  \n",
    "\n",
    "This assignment assumes some familiarity with the framework. You can consult `pytorch`'s [documentation](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) to brush up on your knowledge.\n",
    "\n",
    "\n",
    "The main goals of this assignment are:  \n",
    "  - Train a BPE-based tokenizer to transform input from raw textual representation in an adequate one that can serve as input for the neural model.\n",
    "  - Use Gated Recurrent Units (GRU) as building blocks to create a seq2seq model for neural machine translation.\n",
    "  - Build different variations of seq2seq models for French-to-English translation.\n",
    "  - Evaluate said models using machine-translation metrics such as BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Before you can start, make sure you install the dependencies list in the `requirements.txt` file. It is advised that you use a Python virtual enviornment. You can consult how you can a add virtual enviornment to Jupyter's kernal through this [guide](https://janakiev.com/blog/jupyter-virtual-envs/).  \n",
    "In this assignment you will train two seq2seq models for a duration $25K$ steps (not epochs). On a Mabook Pro with an M2 chip, the training time took $\\sim 50$ minutes and $90$ minutes respectively. If you do not have access to enough computational resources here are your options:\n",
    "- Use [Google Colab](https://colab.research.google.com/).\n",
    "- Use Dal's Brookside and Calvert [servers](https://www.dal.ca/faculty/computerscience/for-faculty-staff/technical-services.html). Check under _Academic compute enviornment_ > _GPU Processing_. There you will find links that redirect you to the Jupyter lab interface hosted on these servers.\n",
    "- Reduce the number of training step.\n",
    "\n",
    "**Please note that your grade will depend on the implementation and not on the running results. Hence, if you cannot train these models fully, you can still get the full grade if your implementation is correct.**\n",
    "\n",
    "## Submission Guidelines\n",
    "Add the TA as _Maintainer_ to your fork and submit your fork's URL on Brightspace. Push the modifications you have made on this notebook to your fork.  \n",
    "**Do NOT alter any of the functions/classes in the Python files of this repository. You are however free to add functions if you need to. If you do, make sure to push the relevant changes**.\n",
    "\n",
    "## Submission of SDAs\n",
    "For this assignment, SDAs should be submitted through Brightspace. You will find the submission box under _Assignments_ > _SDA_. **<span style=\"color:red\">Please note that you are only allowed to submit 2 SDAs for this course. Hence, if you already reached that limit, you can no longer submit one. In addition, no SDA will be considered if sent by email. All SDAs should be submitted through Brightspace prior to the deadline.</span>**  \n",
    "\n",
    "If you have any questions, please do not hesitate to post them on the courseâ€™s Teams channel. If you have not joined already, you can do so by using this code: `ot45hmp`.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WooCc-7gqriU",
    "outputId": "3f283cb8-0d75-4fdc-e3af-c29e1781bebd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = os.path.join(\".\", \"data\")\n",
    "bpe_files = os.path.join('.', \"bpe_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmDQjbvPTfdM"
   },
   "source": [
    "## Part 1: Background and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w-g_de-jZVQ"
   },
   "source": [
    "The dataset contains $\\sim 259K$ pairs of French and English sequences. It is taken from the work of [(Tiedemann, 2012)](https://aclanthology.org/L12-1246/). The dataset is stored in `data/data.txt`. Each line follows this form: `<ENGLISH-SENTENCE> \\t <FRENCH-SENTENCE>`.  \n",
    "First, we need to create a vocabulary using Byte Pair Encoding.  \n",
    "\n",
    "Byte Pair Encoding (BPE) is a data compression technique adapted for use in neural machine translation to handle the vocabulary of words in a text corpus. In NMT, we use BPE to segment words into subword units (_i.e._, tokens), which helps in managing rare and out-of-vocabulary words by breaking them down into smaller, more frequent components. For instance, instead of treating the word `eating` as a part of the vocabulary, we subdivide into `eat` and `ing` and treat these tokens as part of the vocab.\n",
    "\n",
    "Here is how BPE works:\n",
    "\n",
    "1. **Initialization**: Start with a vocabulary of individual characters.\n",
    "2. **Frequency Counting**: Count the frequency of each pair of adjacent symbols (initially characters) in the text.\n",
    "3. **Merging**: Replace the most frequent pair with a new symbol and add it to the vocabulary.\n",
    "4. **Iteration**: Repeat the counting and merging steps until a predefined vocabulary size is reached or no more pairs can be merged.\n",
    "\n",
    "Iteratively, BPE creates a set of subword units that can represent any word in the text, even if it was not seen during training. This approach helps in reducing the vocabulary size while maintaining the ability to generate and understand rare or complex words, improving the efficiency and performance of neural machine translation models.  \n",
    "\n",
    "During the lecture you will get into more details how BPE works. For now, all you need to know is BPE is a way to construct a vocabulary given a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjUnfud-q3aY"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "\n",
    "Your first task is to read the `data/data.txt` file and create two files: `data/train.en`, `data/train.fr`. In the `train.en` file each line should represent the English sentence extracted from the pairs stored in `data/data.txt` after applying the following operations: \n",
    "1. Convert the string into lowercase.\n",
    "2. Tokenize the string using the `WordPunctTokenizer` class into a list of tokens.\n",
    "3. Convert the list of tokens back into one string using whitespace as a delimiter.\n",
    "\n",
    "Same logic applies for `train.fr`, but for French sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you perform the instructions mentioned above, execute the following cell which is responsible of:\n",
    "1. Learning a set of merges using BPE.\n",
    "2. Creating a vocabulary of size $8000$.\n",
    "3. Converting the `train.en` and `train.fr` files into tokenized samples using BPE and the learned merges and storing the results into `train.bpe.en` and `train.bpe.fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7u-RjqrAnuQi",
    "outputId": "17ffc763-41a1-45c2-bc44-43c1fa28a61b"
   },
   "outputs": [],
   "source": [
    "# build and apply bpe vocs\n",
    "bpe = {}\n",
    "for lang in ['en', 'fr']:\n",
    "    learn_bpe(open(os.path.join(data_path, f'train.{lang}')), open(os.path.join(bpe_files, f'bpe_rules.{lang}'), 'w'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open(os.path.join(bpe_files, f'bpe_rules.{lang}')))\n",
    "\n",
    "    with open(os.path.join(data_path, f'train.bpe.{lang}'), 'w') as f_out:\n",
    "        for line in open(os.path.join(data_path, f'train.{lang}')):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well, you should find the two files `train.bpe.en` and `train.bpe.fr` created in the `data` folder.  \n",
    "\n",
    "Now, we will convert the raw tokens into numerical format and create training and testing sets. This will be achieved through `numpy` and the `Vocab` class located in the `vocab.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CseQPY-hrITj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnutfcDwsjZt",
    "outputId": "9510b038-4cc4-49b8-c719-b1b7bceaae45"
   },
   "outputs": [],
   "source": [
    "data_inp = np.array(open(os.path.join(data_path, 'train.bpe.fr')).read().split('\\n'))\n",
    "data_out = np.array(open(os.path.join(data_path, 'train.bpe.en')).read().split('\\n'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGpRh8uit7b4"
   },
   "outputs": [],
   "source": [
    "from vocab import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8I76ii1ugwL",
    "outputId": "3b0343e1-3114-4a3f-a416-b1d003ccee9a"
   },
   "outputs": [],
   "source": [
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFFsXPUbUStF"
   },
   "source": [
    "You can see from above that input batch (the variable `batch_ids`) is of dimension $(B\\times seq\\_len)$, where $B$ is the number of samples and $seq\\_len$ is the length of each sample. In addition, you can notice how each sample starts with `[BOS]` token and ends with an `[EOS]` token. Some samples are padded to have a length of $seq\\_len$ by adding multiple `[EOS]` tokens on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below, will plot the distribution of sequence lengths of the source and target samples. We do this to get an idea regarding the training and inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ngZYUM4wuyLj",
    "outputId": "67066662-708b-4191-b5aa-596d7bff918f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6ber8vYTzh9"
   },
   "source": [
    "## Part 2: Bulding the seq2seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq7fdE9iT5Nl"
   },
   "source": [
    "During the lecture, you have encountered a variant of RNNs called Long Short-Term Memory (LSTM) that addresses some of the limitations that are present in vanilla RNNs.  \n",
    "In this assignment, you will use another type of RNNs: Gated Recurrent Units or GRUs. The GRU bears a similarity with LSTM, in the sense that it also implements a gating mechanism. However, it does not contain a context vector or an output gate, making it smaller, in terms of parameter count, compared to the LSTM. If you are interested in the interals of such layer, you can refer to the following [resource](https://d2l.ai/chapter_recurrent-modern/gru.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlHjJ8tOvFuz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRN8BBfiV9Ym"
   },
   "source": [
    "### GRU-based seq2seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KregU0kEWA9L"
   },
   "source": [
    "As we have mentioned earlier, you will develop GRU-based seq2seq model that translates French sequences to English. Concretely, the model is composed of two components: an encoder and a decoder. This model translates sequences (e.g., sentences) from an input language to an output language. Hereâ€™s the high-level workflow:  \n",
    "\n",
    "1. Input Processing (Encoder): \n",
    "   - Input tokens are converted into dense vector representations or embeddings using an embedding layer.  \n",
    "   - Then, a **GRU-based encoder** processes the entire input sequence into a _context vector_ (final hidden state).  \n",
    "\n",
    "2. Decoding Initialization: \n",
    "   - The encoderâ€™s final hidden state is transformed into an initial state for the decoder.  \n",
    "\n",
    "3. Output Generation (Decoder):  \n",
    "   - The decoder uses a **GRUCell** to generate output tokens **step-by-step**:  \n",
    "     - At each step, it embeds the previous output token (or the start token initially).  \n",
    "     - The GRU cell updates its hidden state using this embedding and the previous state.  \n",
    "     - A linear layer maps the updated state to *logits* (scores) over the output vocabulary.  \n",
    "   - During **training**, the decoder uses reference output tokens.  \n",
    "   - During **inference**, it greedily selects the highest-scoring token at each step.  \n",
    "\n",
    "---\n",
    "\n",
    "## Tasks \n",
    "1. Complete the `__init__` method of a simple encoder-decoder sequence-to-sequence model in PyTorch:  \n",
    "    1. Use embedding layers for both input and output vocabularies.  \n",
    "    2. Include a **single-layer GRU encoder**.  \n",
    "    3. Initialize a decoder that:  \n",
    "       - Starts with a hidden state derived from the encoderâ€™s final state via a linear layer.  \n",
    "       - Uses a **GRUCell** for step-wise token generation.  \n",
    "       - Maps decoder hidden states to scores over the output vocabulary _i.e._ logits.\n",
    "\n",
    "**N.B:** Make sure that you do NOT change the name of the variable in the `__init__` function. If you do so, the code would break.\n",
    "\n",
    "2. Complete the implementation of `decode_step` function that generates the next deoder states. This function will be used by `decode` method to generate output tokens during training. Similarly, it will be used by the `decode_inference` method during inference.\n",
    "\n",
    "You can consult the documentation of the following layers:\n",
    "- [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
    "- [`nn.GRU`](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html). Note that you need to set `batch_first=True` so that this layer process batches.\n",
    "- [`nn.GRUCell`](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html).\n",
    "- [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f249VoyvNrH"
   },
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        A simple encoder-decoder seq2seq model\n",
    "        \"\"\"\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.emb_inp = ...\n",
    "        self.emb_out = ...\n",
    "        self.enc0 = ...\n",
    "\n",
    "        self.dec_start = ...\n",
    "        self.dec0 = ...\n",
    "        self.logits = ...\n",
    "\n",
    "    def forward(self, inp, out):\n",
    "        \"\"\" Apply model in training mode \"\"\"\n",
    "        initial_state = self.encode(inp)\n",
    "        return self.decode(initial_state, out)\n",
    "\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :returns: initial decoder state tensors, one or many\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "\n",
    "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "\n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "\n",
    "        dec_start = self.dec_start(last_state)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
    "        \"\"\"\n",
    "        prev_gru0_state = prev_state[0]\n",
    "\n",
    "        ## Your code goes gere ##\n",
    "        # Embed the previous tokens\n",
    "        ...\n",
    "\n",
    "        # Update the GRU state\n",
    "        ...\n",
    "\n",
    "        # Compute output logits\n",
    "        ...\n",
    "\n",
    "        # Prepare the new state (which is a list containing only the new GRU state)\n",
    "        ...\n",
    "       \n",
    "        return ..., ...\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
    "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "\n",
    "        logits_sequence = [first_logits]\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
    "            logits_sequence.append(logits)\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
    "                              device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "\n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is for sanity check. If all assertions pass, then most likely your implementation is correct. If not, go back and see if you have missed something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bUKF3ifwkYj"
   },
   "outputs": [],
   "source": [
    "# debugging area\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "assert logits_seq2.shape == logits_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VsYZCx6woDz",
    "outputId": "ecc88057-b827-4969-ee7c-c9a91b430567"
   },
   "outputs": [],
   "source": [
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
    "print(\"Translations without training:\")\n",
    "print('\\n'.join([line for line in dummy_translations]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwM5zHeeVwiD"
   },
   "source": [
    "### Training loss\n",
    "\n",
    "Our training objective is almost the same as it was for neural language models:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but **excluding** PAD, and $\\theta$ represents the model's weights.\n",
    "\n",
    "For each target sequence $(Y = [\\text{BOS}, y_1, y_2, ..., \\text{EOS}])$, the model predicts the probability $p(y_t \\mid y_1, \\dots, y_{t-1}, X)$ for every token $y_t$ in $Y$, starting from $\\text{BOS}$.  \n",
    "\n",
    "Through the minimization of this loss, the model learns to assign high probabilities to the correct tokens in the target sequence, conditioned on both the input $X$ and the autoregressive history $(y_1, \\dots, y_{t-1})$. This aligns with the goal of accurate, end-to-end sequence generation.\n",
    "\n",
    "## Tasks:\n",
    "- Complete the implementation of the `compute_loss` function following the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PMSiHcuwsLT"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param model: the seq2seq model\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "\n",
    "    In order to pass the tests, your function should\n",
    "    * include loss at first EOS but not the subsequent ones\n",
    "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
    "    \"\"\"\n",
    "\n",
    "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
    "\n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = ...\n",
    "\n",
    "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
    "    logprobs_seq = ...\n",
    "\n",
    "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
    "    logp_out = ...\n",
    "    # ^-- this will select the probability of the actual next token.\n",
    "\n",
    "    # Compute the loss only where mask is True\n",
    "    masked_loss = ...\n",
    "\n",
    "    # Sum the losses\n",
    "    total_loss = ...\n",
    "\n",
    "    # Compute the total number of tokens (sum of sequence lengths)\n",
    "    total_tokens = ...\n",
    "\n",
    "    # Average cross-entropy over tokens where mask == True\n",
    "    average_loss = ...\n",
    "\n",
    "    return average_loss  # average loss, scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to perform a sanity check of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dX1KeJYIxmz2",
    "outputId": "77f68c01-1e35-4662-ac92-18094c842125"
   },
   "outputs": [],
   "source": [
    "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print(\"Loss:\", dummy_loss)\n",
    "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), \"We're sorry for your loss\"\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJc6AePFYSQk"
   },
   "source": [
    "### Evaluation using BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) measures how well a machine-generated translation matches one or more human reference translations. It computes a score between 0 (no overlap) and 100 (perfect match) by:  \n",
    "\n",
    "- N-gram Precision: Checking how many word/phrase fragments (n-grams) in the candidate translation appear in the references.  \n",
    "\n",
    "- Brevity Penalty: Penalizing overly short translations to avoid inflated scores from trivial matches.\n",
    "\n",
    "The code below computes BLEU at the corpus level (across all sentences), with smoothing to handle edge cases (e.g., missing n-grams). The higher BLEU score is the better translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDxFUTDIxqTH"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\"\n",
    "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
    "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "        return corpus_bleu(\n",
    "            [[ref.split()] for ref in actual],\n",
    "            [trans.split() for trans in translations],\n",
    "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "            ) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute below to get the BLEU score of the current untrained model on the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FV5lKvOOxyec",
    "outputId": "fc417af3-8037-4fb7-c5da-8c000d2b2b3e"
   },
   "outputs": [],
   "source": [
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the seq2seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything ready to train the NMT model. The training loop will render the training loss and BLEU score values on the evaluation set every $100$ steps. If your implementation is correct, you should see a decreasing training loss curve and increasing BLEU score curve. Execute the two cells below to start the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyO4ruHIxz_Y"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "z-koV66GyIDq",
    "outputId": "cfa62aa5-73ab-4f3a-a898-b85a777ad71a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(range(25000))\n",
    "for _ in pbar:\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "\n",
    "    # Training step\n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "\n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PfHH0J2ZfzA"
   },
   "source": [
    "## Part 3: Adding Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0uVrnYIZmse"
   },
   "source": [
    "Now that you have built a functional vanilla seq2seq model, such model has some limitationsâ€”particularly in handling long sequences or retaining fine-grained input context. To address this, you will enhance the decoder by integrating an attention mechanism. Unlike the vanilla model, which relies solely on a fixed context vector from the encoder, attention allows the decoder to dynamically \"focus\" on relevant parts of the input sequence at each decoding step. This step equips the model with the ability to better align input and output sequences, improving translation quality and robustness. Your task is to implement the attention layer and integrate it into the existing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx8KHRMYbuP4"
   },
   "source": [
    "### Creating an Attention Layer\n",
    "\n",
    "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$, input mask and a single decoder state $h^d$,\n",
    "\n",
    "* Compute logits with a 2-layer neural network\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Apply input mask to the logits so that their probabilities is set to 0 later\n",
    "$$a_t = -\\inf \\text{for masked/padded encoder positions}$$\n",
    "* Get probabilities from logits,\n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Add up encoder states with probabilities to get __attention response__\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Your task is to complete the implementation of the `__init__` and `forward` methods of the `AttentionLayer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOHBT6hRyOb0"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, layer_name: str, enc_size, dec_size, hid_size, activ=torch.tanh):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size # num units in encoder state\n",
    "        self.dec_size = dec_size # num units in decoder state\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        # create trainable parameters\n",
    "        ## Your code goes here\n",
    "        self.linear_e = ...\n",
    "        self.linear_d = ...\n",
    "        self.linear_out = ...\n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute logits\n",
    "        enc_proj = ...  # [batch_size, ninp, hid_size]\n",
    "        dec_proj = ...  # [batch_size, 1, hid_size]\n",
    "\n",
    "        # Apply activation function\n",
    "        hidden = ...  # [batch_size, ninp, hid_size]\n",
    "\n",
    "        logits = ...  # [batch_size, ninp]\n",
    "\n",
    "        # Apply mask. Hint check torch.where\n",
    "        masked_logits = ...\n",
    "\n",
    "        # Compute attention probabilities\n",
    "        probs = ...  # [batch_size, ninp]\n",
    "\n",
    "        # Compute attention response using enc and probs\n",
    "        attn = ...  # [batch_size, enc_size]\n",
    "\n",
    "        return attn, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OsNh7XMe2y9"
   },
   "source": [
    "### Integrating the Attention Layer in the seq2eq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rppm0G6CfLN3"
   },
   "source": [
    "The attention layer will be used in the decoder part. On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.\n",
    "\n",
    "### Tasks\n",
    "1. Complete the `__init__` method of the attention-based encoder-decoder sequence-to-sequence model in PyTorch:  \n",
    "    1. The structure is the same as the `BasicModel` class, but adds an `AttentionLayer` as an extra component. \n",
    "         \n",
    "2. Complete the implementation of the `encode` and `decode_step` functions so that the attention mechanism is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3JjWoL8fz9C"
   },
   "outputs": [],
   "source": [
    "class AttentiveModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128, attn_size=128):\n",
    "        super().__init__()\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.emb_inp = ...\n",
    "        self.emb_out = ...\n",
    "        self.enc = ...\n",
    "\n",
    "        self.dec = ...\n",
    "        self.attn = ...\n",
    "        self.out = ...\n",
    "\n",
    "    def forward(self, inp, out):\n",
    "        \"\"\" Apply model in training mode \"\"\"\n",
    "        initial_state = self.encode(inp)\n",
    "        return self.decode(initial_state, out)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        enc_seq, _ = self.enc(inp_emb)\n",
    "\n",
    "        # Create encoder mask\n",
    "        inp_mask = (inp != self.inp_voc.eos_ix).float()\n",
    "\n",
    "        # Initial decoder hidden state (last encoder state)\n",
    "        dec_hidden = ...\n",
    "\n",
    "        # Apply attention layer from initial decoder hidden state\n",
    "        first_attn, first_attn_probas = ...\n",
    "\n",
    "        # Build first state\n",
    "        first_state = [dec_hidden, enc_seq, inp_mask, first_attn, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        prev_hidden, enc_seq, inp_mask, prev_attn, prev_attn_probas = prev_state\n",
    "\n",
    "        # Embed previous tokens\n",
    "        prev_emb = ...\n",
    "\n",
    "        # Concatenate previous embedding and attention response\n",
    "        dec_input = ...\n",
    "\n",
    "        # Run decoder GRU\n",
    "        new_hidden = ...\n",
    "\n",
    "        # Apply attention\n",
    "        attn, attn_probas = ...\n",
    "\n",
    "        # Compute output logits\n",
    "        output_logits = ...\n",
    "\n",
    "        new_state = [new_hidden, enc_seq, inp_mask, attn, attn_probas]\n",
    "        return new_state, output_logits\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
    "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "\n",
    "        logits_sequence = [first_logits]\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
    "            logits_sequence.append(logits)\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
    "                              device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "\n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtLLizOiir-D"
   },
   "source": [
    "### Training the new seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHWCqMxkivOq"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "D2vkgkD-iy6X",
    "outputId": "6e843129-3883-4757-e7f2-ccf96d3b724c"
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(range(25000))\n",
    "for _ in pbar:\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "\n",
    "    # Training step\n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "\n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the implementation is correct, the training behaviour of `AttentiveModel` should be similar to that of the `BasicModel`. The difference is that the `AttentiveModel` model would have a faster convergence, _i.e._, the training loss decreases faster, and the BLEU score would be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VEpT2xok5F9"
   },
   "source": [
    "## Part 4: Rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that partial grading will be handled on a case by case manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (/0.5):\n",
    "- Preprocessing `train.en`: /0.25 pt\n",
    "- Preprocessing `train.fr`: /0.25 pt\n",
    "\n",
    "## Part 2 (/6):\n",
    "### Basic seq2seq:\n",
    "- Implementation of the `__init__` function: /2\n",
    "- Implementation of the `decode_inference` function: /2\n",
    "- Implementation of the loss function: /2\n",
    "\n",
    "## Part 3 (/7.5):\n",
    "\n",
    "### Attention layer:\n",
    "- Implementation of the `__init__` function: /1\n",
    "- Implementation of the `forward` function: /2\n",
    "\n",
    "### Attention-based seq2seq:\n",
    "- Completion of `__init__` function: /0.5\n",
    "- Completion of `encode function`: /2\n",
    "- Completion of `decode_step` function: /2\n",
    "\n",
    "\n",
    "### Total: /14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "a3",
   "language": "python",
   "name": "a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
